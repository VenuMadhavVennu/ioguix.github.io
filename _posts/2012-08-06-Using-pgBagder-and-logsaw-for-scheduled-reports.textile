---
layout: post
author: ioguix
title: Using pgBagder and logsaw for scheduled reports
date: 2012-08-06 15:37 +0200
tags: [postgresql, pgbadger, logsaw]
category: postgresql
---
p. Hey,

p. While waiting for next version of pgBadger, here is a tip to create scheduled pgBadger report.  For this demo, I'll suppose :
* we have PostgreSQL's log files in <code>/var/log/pgsql</code>
* we want to produce a weekly report using pgBadger with the "postgres" system user...
* ...so we keep at least 8 days of log

p. You will need "pgbadger":https://github.com/dalibo/pgbadger/downloads and "logsaw":https://github.com/dalibo/logsaw/downloads.  Both tools are under BSD license and pure perl script with no dependencies.

p. "logsaw" is a tool aimed to parse log files, looking for some regexp-matching lines, printing them to standard output and remembering where it stop last time.  At start, it searches for the last line it parsed during its last call, and starts working from there.  Yes, for those familiar with "tail_n_mail", it does the same thing, but without the mail and report processing part.  Moreover, it supports rotation and compression of log files (not sure about tail_n_mail).  Thanks to this tool, we'll be able to create new reports from where the last one finished!

p. We need to create a simple configuration file for logsaw:

{% highlight bash %}
cat <<EOF > ~postgres/.logsaw
LOGDIR=/var/log/pgsql/
EOF
{% endhighlight %}

p. There's three more optional parameters in this configuration file you might want to know:

* if you want to process some particular files, you can use the <code>LOGFILES</code> parameter, a regular expression to filter files in the <code>LOGDIR</code> folder. When not defined, the default empty string means: «take all files in the folder».
* if your log files are compressed, you can use the <code>PAGER</code> parameter which should be a command that uncompress your log files to standard output, eg. <code>PAGER=zcat -f</code>. Note that if available, logsaw will use silently IO::Zlib to read your compressed files.
* you can filter extracted lines from log files using the <code>REGEX</code> parameters. You can add as many <code>REGEX</code> parameter than needed, each of them must be a regular expression. When not defined, the default empty <code>REGEX</code> array means: «match all the lines».
* each time you call logsaw, it saves its states to your configuration file, adding parameters <code>FILEID</code> and <code>OFFSET</code>.

p. That's it for logsaw. See its README files for more details, options and sample configuration file.

Now, the command line to create the report using pgbadger:

{% highlight bash %}
logsaw | pgbadger -g -o /path/to/report-$(date +%Y%m%d).html -
{% endhighlight %}

p. You might want to add the <code>-f</code> option to pgbadger if it doesn't guess the log format itself (stderr or syslog or csv).

About creating reports on a weekly basis, let's say every sunday at 2:10am, using crontab:
{% highlight bash %}
10 2 1-7 * 7  logsaw | pgbadger -g -o /path/to/report-$(date +\%Y\%m\%d).html -
{% endhighlight %}

p. Here you go, enjoy :)

p. That's why I like UNIX style and spirit commands: simple single-task but powerful and complementary tools.

p. Wait or help for more nice features in pgBadger !

p. Cheers !

p. PS: There's another tool to deal with log files and reports you might be interested in: "logwatch"
